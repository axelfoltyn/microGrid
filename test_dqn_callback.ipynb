{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc926df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\WorkSpace\\code\n",
      "C:\\WorkSpace\\code\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from joblib import hash, dump, load\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "print(os.path.abspath(''))\n",
    "from os import path\n",
    "sys.path.append( path.abspath(os.path.abspath('')) ) \n",
    "print(path.abspath(os.path.abspath('')) )\n",
    "\n",
    "from deer.default_parser import process_args\n",
    "from microGrid.env.final_env import MyEnv as MG_two_storages_env\n",
    "import microGrid.experiment.base_controllers as bc\n",
    "from datetime import datetime\n",
    "from microGrid.plot_MG_operation import plot_op\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f762bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow work with: \n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow work with:\", tf.test.gpu_device_name())\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "rng = np.random.RandomState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ec88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defaults:\n",
    "    # ----------------------\n",
    "    # Experiment Parameters\n",
    "    # ----------------------\n",
    "    STEPS_PER_EPOCH = 365*24-1\n",
    "    EPOCHS = 200\n",
    "    STEPS_PER_TEST = 365*24-1\n",
    "    PERIOD_BTW_SUMMARY_PERFS = -1  # Set to -1 for avoiding call to env.summarizePerformance\n",
    "    \n",
    "    # ----------------------\n",
    "    # Environment Parameters\n",
    "    # ----------------------\n",
    "    FRAME_SKIP = 1\n",
    "\n",
    "    # ----------------------\n",
    "    # DQN Agent parameters:\n",
    "    # ----------------------\n",
    "    UPDATE_RULE = 'rmsprop'\n",
    "    LEARNING_RATE = 0.02\n",
    "    LEARNING_RATE_DECAY = 0.99\n",
    "    DISCOUNT = 0.99\n",
    "    DISCOUNT_INC = 0.99\n",
    "    DISCOUNT_MAX = 0.98\n",
    "    RMS_DECAY = 0.9\n",
    "    RMS_EPSILON = 0.0001\n",
    "    MOMENTUM = 0\n",
    "    CLIP_NORM = 1.0\n",
    "    EPSILON_START = 1.0\n",
    "    EPSILON_MIN = .1\n",
    "    EPSILON_DECAY = 2.3e-5\n",
    "    UPDATE_FREQUENCY = 1\n",
    "    REPLAY_MEMORY_SIZE = 1000000\n",
    "    BATCH_SIZE = 32\n",
    "    FREEZE_INTERVAL = 1000\n",
    "    DETERMINISTIC = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dafbd227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate reward_function ---\n",
    "price_h2 = 0.1  # 0.1euro/kWh of hydrogen\n",
    "price_elec_buy = 2.0  # 2euro/kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7261cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce_qty_data, length_history, start_history\n",
      "1 12 0\n",
      "Sample of the consumption profile (kW): [4.88090280e-05 4.08636652e-04 2.66459683e-03 1.35325348e-02\n",
      " 5.35289841e-02 1.64925998e-01 3.95869422e-01 7.40576948e-01\n",
      " 1.08118010e+00 1.23674331e+00 1.12378457e+00 8.51875992e-01\n",
      " 6.28731540e-01 5.94891051e-01 7.62627204e-01 1.05973501e+00\n",
      " 1.38386090e+00 1.63210044e+00 1.72495565e+00 1.63169652e+00\n",
      " 1.38119778e+00 1.04620892e+00 7.09130415e-01 4.30109338e-01]\n",
      "Min of the consumption profile (kW): 3.569456443385281e-05\n",
      "Max of the consumption profile (kW): 2.1\n",
      "Average consumption per day train (kWh): 18.56458726184315\n",
      "self.production_train brefore\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "self.production_train after\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Sample of the production profile (kW): [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00225469 0.01691014 0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Min of the production profile (kW): 0.0\n",
      "Max of the production profile (kW): 11.981962618953437\n",
      "Average production per day train (kWh): 35.093446652096034\n",
      "reduce_qty_data, length_history, start_history\n",
      "1 12 0\n",
      "Sample of the consumption profile (kW): [5.12109060e-05 4.28745542e-04 2.79572086e-03 1.41984669e-02\n",
      " 5.61631298e-02 1.73041959e-01 4.15350040e-01 7.77020523e-01\n",
      " 1.13438465e+00 1.29760308e+00 1.17908568e+00 8.93796561e-01\n",
      " 6.59671235e-01 6.24165466e-01 8.00155866e-01 1.11188426e+00\n",
      " 1.45196029e+00 1.71241563e+00 1.80984022e+00 1.71199183e+00\n",
      " 1.44916612e+00 1.09769256e+00 7.44026516e-01 4.51274893e-01]\n",
      "Min of the consumption profile (kW): 3.5681312703728945e-05\n",
      "Max of the consumption profile (kW): 2.0948896439640574\n",
      "Average consumption per day train (kWh): 18.419244275842022\n",
      "self.production_train brefore\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "self.production_train after\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Sample of the production profile (kW): [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.17699281 0.22715957\n",
      " 0.30776458 0.23054159 0.15895533 0.06144018 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Min of the production profile (kW): 0.0\n",
      "Max of the production profile (kW): 12.0\n",
      "Average production per day train (kWh): 35.91250439789233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkSpace\\code\\venv\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate environment ---\n",
    "env = MG_two_storages_env(rng)\n",
    "absolute_dir = os.path.abspath('')\n",
    "prod = np.load(absolute_dir + \"/microGrid/env/data/BelgiumPV_prod_test.npy\")[0:1 * 365 * 24]\n",
    "cons = np.load(absolute_dir + \"/microGrid/env/data/example_nondeterminist_cons_test.npy\")[0:1 * 365 * 24]\n",
    "env_valid = MG_two_storages_env(rng, consumption=cons, production=prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2659e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.add_reward(\"flow_h2\", lambda x: x[\"flow_H2\"] * price_h2, 1.)\n",
    "env.add_reward(\"buy_energy\", lambda x: -x[\"lack_energy\"] * price_elec_buy, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa9cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_valid.add_reward(\"flow_h2\", lambda x: x[\"flow_H2\"] * price_h2, 1.)\n",
    "env_valid.add_reward(\"buy_energy\", lambda x: -x[\"lack_energy\"] * price_elec_buy, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "047be8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MlpPolicy\n",
      "learning_rate=\n",
      "0.02\n",
      "buffer_size=\n",
      "1000000\n",
      "batch_size=\n",
      "32\n",
      "gamma=\n",
      "0.99\n",
      "exploration_initial_eps=\n",
      "1.0\n",
      "exploration_final_eps=\n",
      "0.1\n",
      "exploration_fraction=\n",
      "2.3e-05\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "print('MlpPolicy', \n",
    "            \"learning_rate=\",Defaults.LEARNING_RATE, \n",
    "            \"buffer_size=\",Defaults.REPLAY_MEMORY_SIZE, \n",
    "            \"batch_size=\",Defaults.BATCH_SIZE, \n",
    "            \"gamma=\",Defaults.DISCOUNT,\n",
    "            \"exploration_initial_eps=\",Defaults.EPSILON_START,\n",
    "            \"exploration_final_eps=\",Defaults.EPSILON_MIN,\n",
    "            \"exploration_fraction=\",Defaults.EPSILON_DECAY,sep='\\n')\n",
    "model = DQN('MlpPolicy', env, \n",
    "            learning_rate=Defaults.LEARNING_RATE, \n",
    "            buffer_size=Defaults.REPLAY_MEMORY_SIZE, \n",
    "            batch_size=Defaults.BATCH_SIZE, \n",
    "            gamma=Defaults.DISCOUNT,\n",
    "            exploration_initial_eps=Defaults.EPSILON_START,\n",
    "            exploration_final_eps=Defaults.EPSILON_MIN,\n",
    "            exploration_fraction=Defaults.EPSILON_DECAY,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40a70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "#callback\n",
    "class VerboseCallback(BaseCallback):\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "    \n",
    "\n",
    "class BestCallback(BaseCallback):\n",
    "    def __init__(self, env_valid, patience, dirname, parent_path):\n",
    "        super().__init__()\n",
    "        self.validationScores = []\n",
    "        self.trainScores = []\n",
    "        self.env_valid = env_valid\n",
    "        self.bestValidationScoreSoFar = None\n",
    "        self.cycle = 0\n",
    "        self.patience = patience\n",
    "        self.dirname = dirname\n",
    "        self.parent_path = parent_path\n",
    "        self.data = None\n",
    "    \n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        if not all(self.locals[\"dones\"]):\n",
    "            return True\n",
    "        self.cycle += 1\n",
    "        \n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.env_valid, n_eval_episodes=1)\n",
    "        self.validationScores.append(mean_reward)\n",
    "        \n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.training_env, n_eval_episodes=1)\n",
    "        self.trainScores.append(mean_reward)\n",
    "        \n",
    "        # part best\n",
    "        if self.bestValidationScoreSoFar is None or self.validationScores[-1] > self.bestValidationScoreSoFar:\n",
    "            self.cycle = 0\n",
    "            self.bestValidationScoreSoFar = self.validationScores[-1]\n",
    "            print(\"new best\", self.dirname + \"score:\"+ str(self.validationScores[-1]))\n",
    "            self.model.save(self.parent_path + \"/\" + self.dirname + \"/\" + self.dirname + \n",
    "                            \"-score-\" + str(self.validationScores[-1]))\n",
    "            self.data = self.env_valid.get_data()[-2] #-1 empty because env is reset at the end\n",
    "            #agent.dumpNetwork(self.dirname + \"/\" + self.dirname + \"-score-\" + str(validationScores[-1]), epoch)\n",
    "        if self.cycle >= self.patience:\n",
    "            return False\n",
    "        return True\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "    def get_score(self):\n",
    "        return self.validationScores, self.trainScores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c32e80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkSpace\\code\\venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best best22_04_2022-11-03-26score:-2874.808621198965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WorkSpace\\code\\venv\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:276: UserWarning: Path 'result\\best22_04_2022-11-03-26' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -9.71e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 1474      |\n",
      "|    time_elapsed     | 23        |\n",
      "|    total_timesteps  | 35036     |\n",
      "-----------------------------------\n",
      "new best best22_04_2022-11-03-26score:-2104.543670715915\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -8.11e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 1102      |\n",
      "|    time_elapsed     | 63        |\n",
      "|    total_timesteps  | 70072     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.176     |\n",
      "|    n_updates        | 5017      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -6.58e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 905       |\n",
      "|    time_elapsed     | 116       |\n",
      "|    total_timesteps  | 105108    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.404     |\n",
      "|    n_updates        | 13776     |\n",
      "-----------------------------------\n",
      "new best best22_04_2022-11-03-26score:-1568.6368981439591\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -5.74e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 825       |\n",
      "|    time_elapsed     | 169       |\n",
      "|    total_timesteps  | 140144    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.54      |\n",
      "|    n_updates        | 22535     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -5.34e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 754       |\n",
      "|    time_elapsed     | 232       |\n",
      "|    total_timesteps  | 175180    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.774     |\n",
      "|    n_updates        | 31294     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.76e+03 |\n",
      "|    ep_rew_mean      | -5.1e+03 |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 717      |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total_timesteps  | 210216   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.02     |\n",
      "|    loss             | 0.628    |\n",
      "|    n_updates        | 40053    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.93e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 705       |\n",
      "|    time_elapsed     | 347       |\n",
      "|    total_timesteps  | 245252    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 1.08      |\n",
      "|    n_updates        | 48812     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.81e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 692       |\n",
      "|    time_elapsed     | 405       |\n",
      "|    total_timesteps  | 280288    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.732     |\n",
      "|    n_updates        | 57571     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.72e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 681       |\n",
      "|    time_elapsed     | 462       |\n",
      "|    total_timesteps  | 315324    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.575     |\n",
      "|    n_updates        | 66330     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.65e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 672       |\n",
      "|    time_elapsed     | 521       |\n",
      "|    total_timesteps  | 350360    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.459     |\n",
      "|    n_updates        | 75089     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.58e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 664       |\n",
      "|    time_elapsed     | 579       |\n",
      "|    total_timesteps  | 385396    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 1.1       |\n",
      "|    n_updates        | 83848     |\n",
      "-----------------------------------\n",
      "new best best22_04_2022-11-03-26score:-1251.2542356641425\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.52e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 657       |\n",
      "|    time_elapsed     | 639       |\n",
      "|    total_timesteps  | 420432    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.705     |\n",
      "|    n_updates        | 92607     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.49e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 651       |\n",
      "|    time_elapsed     | 699       |\n",
      "|    total_timesteps  | 455468    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.47      |\n",
      "|    n_updates        | 101366    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 8.76e+03  |\n",
      "|    ep_rew_mean      | -4.45e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 644       |\n",
      "|    time_elapsed     | 760       |\n",
      "|    total_timesteps  | 490504    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.02      |\n",
      "|    loss             | 0.722     |\n",
      "|    n_updates        | 110125    |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = datetime.now()\n",
    "# dd_mm_YY-H-M-S\n",
    "dt_string = now.strftime(\"%d_%m_%Y-%H-%M-%S\")\n",
    "dirname = \"result\"\n",
    "filename = \"best\" + dt_string\n",
    "best=  BestCallback(env_valid, 220, filename, dirname)\n",
    "start = time.time()\n",
    "model.learn(Defaults.EPOCHS * Defaults.STEPS_PER_EPOCH, callback=best)#callback=[verbose_callback, eps_callback, best_callback]\n",
    "print(\"time:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40934bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = best.get_data()\n",
    "validationScores, trainScores = best.get_score()\n",
    "print(data.keys())\n",
    "\n",
    "actions= data[\"action\"]\n",
    "consumption = data[\"consumption\"]\n",
    "production = data[\"production\"]\n",
    "rewards = data[\"rewards\"]\n",
    "battery_level= data[\"battery\"]\n",
    "#plot_op(data[\"action\"], data[\"consumption\"], data[\"production\"], data[\"rewards\"], data[\"battery\"], \"test.png\")\n",
    "i = 0\n",
    "plot_op(actions[0+i:100+i],consumption[0+i:100+i],production[0+i:100+i],rewards[0+i:100+i],battery_level[0+i:100+i],\"testplot_winter_.png\")\n",
    "plt.show()\n",
    "i=180*24\n",
    "plot_op(actions[0+i:100+i],consumption[0+i:100+i],production[0+i:100+i],rewards[0+i:100+i],battery_level[0+i:100+i],\"testplot_summer_.png\")\n",
    "plt.show()\n",
    "i=360*24\n",
    "plot_op(actions[0+i:100+i],consumption[0+i:100+i],production[0+i:100+i],rewards[0+i:100+i],battery_level[0+i:100+i],\"testplot_winter2_.png\")\n",
    "plt.show()\n",
    "\n",
    "key = \"flow_H2\"\n",
    "plt.plot(range(31*24), data[key][:31*24], label=key, color='b')\n",
    "key = \"buy_energy\"\n",
    "plt.plot(range(31*24), data[key][:31*24], label=key, color='r')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of hours\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.savefig(dirname + \"/\" + filename + \"_plots.pdf\")\n",
    "plt.show()\n",
    "\n",
    "h = sns.jointplot(x=[battery_level[i] for i in range(len(actions)) if actions[i] == 0],\n",
    "                  y=[consumption[i] - production[i] for i in range(len(actions)) if actions[i] == 0],\n",
    "                  kind=\"hist\")\n",
    "# JointGrid has a convenience function\n",
    "h.set_axis_labels('charge battery', 'demand', fontsize=16)\n",
    "plt.savefig(dirname + \"/\" + filename + \"_plots_action0.pdf\")\n",
    "plt.show()\n",
    "\n",
    "h = sns.jointplot(x=[battery_level[i] for i in range(len(actions)) if actions[i] == 1],\n",
    "                  y=[consumption[i] - production[i] for i in range(len(actions)) if actions[i] == 1],\n",
    "                  kind=\"hist\")\n",
    "# JointGrid has a convenience function\n",
    "h.set_axis_labels('charge battery', 'demand', fontsize=16)\n",
    "plt.savefig(dirname + \"/\" + filename + \"_plots_action1.pdf\")\n",
    "plt.show()\n",
    "\n",
    "h = sns.jointplot(x=[battery_level[i] for i in range(len(actions)) if actions[i] == 2],\n",
    "                  y=[consumption[i] - production[i] for i in range(len(actions)) if actions[i] == 2],\n",
    "                  kind=\"hist\")\n",
    "# JointGrid has a convenience function\n",
    "h.set_axis_labels('charge battery', 'demand', fontsize=16)\n",
    "plt.savefig(dirname + \"/\" + filename + \"_plots_action2.pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.show()\n",
    "demande = [consumption[i] - production[i] for i in range(len(actions))]\n",
    "print(\"demande moyenne : \", np.mean(demande))\n",
    "print(\"demande std : \", np.std(demande))\n",
    "\n",
    "corr = pd.DataFrame.from_dict(data)\n",
    "corr = corr.corr()\n",
    "plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corr, annot = True)\n",
    "plt.savefig(dirname + \"/\" + filename + \"_heatmap.pdf\")\n",
    "plt.show()\n",
    "print(\"reward\", np.sum(data[\"rewards\"]))\n",
    "\n",
    "\n",
    "plt.plot(range(len(validationScores)), validationScores, label=\"validation score\", color='b')\n",
    "plt.plot(range(len(trainScores)), trainScores, label=\"train score\", color='r')\n",
    "#plt.plot(x, np.repeat(testScores, nb_rep), label=\"TS\", color='r')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Number of cycle\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.savefig(dirname + \"/\" + filename + \"_scores.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
